{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNCT Data Read In and Transformation Postprocessing\n",
    "\n",
    "Within this script, manual FNCT data input added using the previously created 'input mask' (applying [FNCT Data Transformation](https://github.com/MarkusSchilling/fnct-data-transformation/blob/main/FNCT_Data_Transformation.ipynb) script) is added to the FNCT Data files. Being completed, the file stack my be used for data mapping in order to obtain RDF data using the [FNCT Data Mapping](https://github.com/MarkusSchilling/fnct-data-transformation/blob/main/FNCT_Data_Mapping.ipynb) script to create a comprehensive knowledge graph. A further step to add entries is necessary because some measurements, such as the actual residual fracture surface, are only conducted in a microscopy process downstream of the FNCT process. Accordingly, this data is available later and must be added separately. \n",
    "Moreover, some calculations are performed using this script such as the determination of the actual tensile stress used which is calculated from the data on the actual residual fracture surface as is usual in FNCT experiments.\n",
    "\n",
    "The working folder path has to be specified manually (see below, second cell): \n",
    "```python \n",
    "folder_path = r'Path_to_your_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from Module import fnct_calculations as calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path where Excel files to be processed are located (format for Windows: folder_path = \"C:\\\\Path\\\\To\\\\Your\\\\Folder\" | using double backslashes to escape special characters)\n",
    "folder_path = r'Path_to_your_folder'\n",
    "# For using the current working folder, the os.getcwd() method can be used\n",
    "# folder_path = os.getcwd()\n",
    "\n",
    "# Check if \"secondary_data\" folder exists, otherwise stop the script since no additional data can be included\n",
    "secondary_data_folder = os.path.join(folder_path, \"secondary_data\")\n",
    "if os.path.exists(secondary_data_folder):\n",
    "    print(f\"Accessing secondary data in '{secondary_data_folder}' folder.\")\n",
    "else:\n",
    "    print(f\"No '{secondary_data_folder}' folder found. No data processing possible.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Check if \"input\" folder exists, otherwise stop the script since no input data can be included\n",
    "input_folder = os.path.join(folder_path, \"input\")\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Accessing input data in '{input_folder}' folder.\")\n",
    "else:\n",
    "    print(f\"No '{input_folder}' folder found. No data processing possible.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Check if \"metadata\" folder exists, otherwise create it\n",
    "metadata_folder = os.path.join(folder_path, \"metadata\")\n",
    "if os.path.exists(metadata_folder):\n",
    "    print(f\"Accessing metadata in '{metadata_folder}' folder.\")\n",
    "else:\n",
    "    print(f\"No '{metadata_folder}' folder found. Metadata will not be updated.\")\n",
    "\n",
    "\n",
    "# Read CSV files to be merged into pandas DataFrames\n",
    "secondary_data_path = os.path.join(secondary_data_folder, 'FNCT_secondary_data.csv')\n",
    "input_data_path = os.path.join(input_folder, 'FNCT_Input_Information.csv')\n",
    "\n",
    "secondary_data = pd.read_csv(secondary_data_path, sep=';', dtype=str)\n",
    "input_data = pd.read_csv(input_data_path, sep=';', dtype=str)\n",
    "\n",
    "# Identify rows with missing data in secondary_data\n",
    "# Columns to check for missing data in secondary_data\n",
    "columns_to_check = ['Specimen_ID', 'Process_ID', 'Material', 'Medium', \n",
    "                    'Residual fracture surface measured AL1', \n",
    "                    'Residual fracture surface measured AL2', \n",
    "                    'Notch depth measured nm']\n",
    "\n",
    "# Find rows with any missing values in these columns\n",
    "missing_rows = secondary_data[columns_to_check].isnull().any(axis=1)\n",
    "\n",
    "# Update missing data in secondary_data from input_data\n",
    "# Iterate over each column in input_data and update corresponding columns in secondary_data\n",
    "for col in input_data.columns:\n",
    "    if col in secondary_data.columns and col != 'Process_ID':  # Exclude 'Process_ID' which is common identifier\n",
    "        secondary_data.loc[missing_rows, col] = input_data.loc[missing_rows, col].values\n",
    "\n",
    "columns_to_calculate = ['Residual fracture surface measured AL', 'Stress measured sigma_L']\n",
    "# Find rows with any missing values in these columns to calculate\n",
    "missing_calculation_rows = secondary_data[columns_to_calculate].isnull().any(axis=1)\n",
    "\n",
    "# Update missing data to calculate in secondary_data from input_data\n",
    "# Iterate over each column and update corresponding columns in secondary_data while using calculation methods and functions\n",
    "for index, row in secondary_data.iterrows():\n",
    "    if missing_calculation_rows[index]: # Check if the row needs calculation (are values already existing)\n",
    "        AL1 = row['Residual fracture surface measured AL1']\n",
    "        AL2 = row['Residual fracture surface measured AL2']\n",
    "        F = row['Force']\n",
    "        \n",
    "        residual_fracture_surface_measured = calc.get_calculation_residual_fracture_surface_measured(AL1, AL2)\n",
    "        secondary_data.at[index, 'Residual fracture surface measured AL'] = residual_fracture_surface_measured\n",
    "        sigma_L = calc.get_actual_stress_measured(F, residual_fracture_surface_measured)\n",
    "        secondary_data.at[index, 'Stress measured sigma_L'] = sigma_L\n",
    "\n",
    "# Write updated secondary_data dataframe to a new CSV file\n",
    "output_file = os.path.join(secondary_data_folder, 'FNCT_secondary_data.csv')\n",
    "secondary_data.to_csv(output_file, sep=';', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in input data\n",
    "# Check if \"input\" folder exists, otherwise stop the script since no input data can be included\n",
    "input_folder = os.path.join(folder_path, \"input\")\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Accessing input data in '{input_folder}' folder.\")\n",
    "    input_data_path = os.path.join(input_folder, 'FNCT_Input_Information.csv')\n",
    "    input_data = pd.read_csv(input_data_path, sep=';', dtype=str)\n",
    "else:\n",
    "    print(f\"No '{input_folder}' folder found. No data processing possible.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Ensure 'Process_ID' is a column before extracting unique values\n",
    "if 'Process_ID' not in input_data.columns:\n",
    "    raise KeyError(\"'Process_ID' column not found in input_data\")\n",
    "\n",
    "# Get unique Process_IDs from the input CSV\n",
    "unique_process_ids = input_data['Process_ID'].unique()\n",
    "# Remove NaN values\n",
    "unique_process_ids = unique_process_ids[~pd.isna(unique_process_ids)]\n",
    "\n",
    "# Set 'Process_ID' as the index for input_data\n",
    "input_data.set_index('Process_ID', inplace=True)\n",
    "\n",
    "# Process each Process_ID and its corresponding metadata file\n",
    "for process_id in unique_process_ids:\n",
    "    # Construct the filename for the metadata file, respectively\n",
    "    metadata_file = os.path.join(metadata_folder, f\"{process_id}_metadata.csv\")\n",
    "    \n",
    "    # Check if the metadata file exists\n",
    "    if os.path.exists(metadata_file):\n",
    "        # Read metadata file into a DataFrame\n",
    "        df_metadata = pd.read_csv(metadata_file, sep=';', dtype=str)\n",
    "\n",
    "        # Get the corresponding row from input_data for the current process_id\n",
    "        if process_id not in input_data.index:\n",
    "            print(f\"Process_ID {process_id} not found in input_data\")\n",
    "            continue\n",
    "        input_row = input_data.loc[process_id]\n",
    "        \n",
    "        # Columns to check for missing data in metadata file\n",
    "        columns_to_check_metadata = ['Funding Party', 'Funding Party ID', 'Grant number']\n",
    "\n",
    "        # Find rows with any missing values in these columns of the metadata file\n",
    "        missing_rows_metadata = df_metadata[columns_to_check_metadata].isnull().any(axis=1)\n",
    "        missing_rows_metadata[0] = False  # Ensure the header row is not considered\n",
    "\n",
    "        if not missing_rows_metadata.any():\n",
    "            print(f\"Metadata file: {metadata_file} does not need to be updated.\")\n",
    "            continue\n",
    "\n",
    "        # Only update rows with missing data\n",
    "        rows_to_update = df_metadata[missing_rows_metadata]\n",
    "\n",
    "        # Update missing data in metadata from input_data\n",
    "        # Iterate over each column in input_data and update corresponding columns in metadata\n",
    "        for col in input_data.columns:\n",
    "            if col in df_metadata.columns:\n",
    "                # Update the metadata DataFrame where data is missing\n",
    "                df_metadata.loc[missing_rows_metadata, col] = df_metadata.loc[missing_rows_metadata, col].fillna(input_row[col])\n",
    "\n",
    "        # Write updated metadata DataFrame back to CSV\n",
    "        df_metadata.to_csv(metadata_file, sep=';', index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Updated metadata file: {metadata_file}\")\n",
    "    else:\n",
    "        print(f\"Metadata file not found for Process_ID: {process_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert standard Funding Party and Grant Number\n",
    "\n",
    "As it may be the case with a lot of FNCT data being obtained in the frame of certain projects or in (in-house) quality control processes, funders / principals / customers and grant numbers / project references may often be the same for a set of experiments. Accordingly, this data does not have to be added by hand, but can be added using the following code by script. \n",
    "Therefore, the fixed data has to be edited in the following code.\n",
    "\n",
    "In this line, the code may be edited:\n",
    "\n",
    "```python\n",
    "# Define the fixed values for the metadata fields\n",
    "fixed_values = {\n",
    "    'Funding Party': '',\n",
    "    'Funding Party ID': '',\n",
    "    'Grant number': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in input data\n",
    "input_folder = os.path.join(folder_path, \"input\")\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Accessing input data in '{input_folder}' folder.\")\n",
    "    input_data_path = os.path.join(input_folder, 'FNCT_Input_Information.csv')\n",
    "    input_data = pd.read_csv(input_data_path, sep=';', dtype=str)\n",
    "else:\n",
    "    print(f\"No '{input_folder}' folder found. No data processing possible.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Ensure 'Process_ID' is a column before extracting unique values\n",
    "if 'Process_ID' not in input_data.columns:\n",
    "    raise KeyError(\"'Process_ID' column not found in input_data\")\n",
    "\n",
    "# Get unique Process_IDs from the input CSV\n",
    "unique_process_ids = input_data['Process_ID'].unique()\n",
    "# Remove NaN values\n",
    "unique_process_ids = unique_process_ids[~pd.isna(unique_process_ids)]\n",
    "\n",
    "# Set 'Process_ID' as the index for input_data\n",
    "input_data.set_index('Process_ID', inplace=True)\n",
    "\n",
    "# Define the fixed values for the metadata fields\n",
    "fixed_values = {\n",
    "    'Funding Party': '',\n",
    "    'Funding Party ID': '',\n",
    "    'Grant number': ''\n",
    "}\n",
    "\n",
    "# Process each Process_ID and its corresponding metadata file\n",
    "for process_id in unique_process_ids:\n",
    "    metadata_file = os.path.join(metadata_folder, f\"{process_id}_metadata.csv\")\n",
    "    \n",
    "    if os.path.exists(metadata_file):\n",
    "        # Read metadata file into a DataFrame\n",
    "        df_metadata = pd.read_csv(metadata_file, sep=';', dtype=str)\n",
    "\n",
    "        # Get the corresponding row from input_data for the current process_id\n",
    "        if process_id not in input_data.index:\n",
    "            print(f\"Process_ID {process_id} not found in input_data\")\n",
    "            continue\n",
    "        input_row = input_data.loc[process_id]\n",
    "        \n",
    "        # Columns to check for missing data in metadata file\n",
    "        columns_to_check_metadata = ['Funding Party', 'Funding Party ID', 'Grant number']\n",
    "\n",
    "        # Find rows with any missing values in these columns of the metadata file\n",
    "        missing_rows_metadata = df_metadata[columns_to_check_metadata].isnull().any(axis=1)\n",
    "        missing_rows_metadata[0] = False  # Ensure the header row is not considered\n",
    "\n",
    "        if not missing_rows_metadata.any():\n",
    "            print(f\"Metadata file: {metadata_file} does not need to be updated.\")\n",
    "            continue\n",
    "\n",
    "        # Update missing data in metadata with fixed values\n",
    "        for col in columns_to_check_metadata:\n",
    "            if col in df_metadata.columns:\n",
    "                df_metadata.loc[missing_rows_metadata, col] = fixed_values[col]\n",
    "\n",
    "        # Write updated metadata DataFrame back to CSV\n",
    "        df_metadata.to_csv(metadata_file, sep=';', index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Updated metadata file: {metadata_file}\")\n",
    "    else:\n",
    "        print(f\"Metadata file not found for Process_ID: {process_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
